{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Titanic Survival Prediction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "You'll use cross validation and a hyperparameter grid search to optimize your machine learning pipeline.  \n",
    "\n",
    "You will use the Titanic Survival Dataset to build a classification model to predict whether a passenger survived the sinking of the Titanic, based on attributes of each passenger in the data set.\n",
    "\n",
    "You'll start with building a Random Forest Classifier, then modify your pipeline to use a Logistic Regression estimator instead. You'll evaluate and compare your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Use scikit-learn to build a model to solve a classification problem\n",
    " - Implement a pipeline to combine your preprocessing steps with a machine learning model\n",
    " - Interpret the results of your modelling\n",
    " - Update your pipeline with a different machine learning model\n",
    " - Compare the preformances of your classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the requirements.txt to install required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Passenger data set\n",
    "We'll be working with the Titanic passenger dataset to build a classification model to predict whether a passenger survied the sinking of the Titanic.  \n",
    "Here is the data dictionary:\n",
    "\n",
    "| Variable   |\tDefinition   |\n",
    " |:------|:--------------------------------|\n",
    " |survived | survived? 0 = No, 1 = yes  |\n",
    " |pclass | Ticket class (int)  |\n",
    " |sex\t |sex |\n",
    " |age\t | age in years  |\n",
    " |sibsp  |\t# of siblings / spouses aboard the Titanic |\n",
    " |parch  |\t# of parents / children aboard the Titanic |\n",
    " |fare   |\tPassenger fare   |\n",
    " |embarked | Port of Embarkation |\n",
    " |class  |Ticket class (obj)   |\n",
    " |who    | man, woman, or child  |\n",
    " |adult_male | True/False |\n",
    " |alive  | yes/no  |\n",
    " |alone  | yes/no  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Titanic dataset using Seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select relevant features and the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features to drop\n",
    "`deck` has a lot of missing values so we'll drop it. `age` has quite a few missing values as well. Although it could be, `embarked` and `embark_town` don't seem relevant so we'll drop them as well. It's unclear what `alive` refers to so we'll ignore it. More detailed Exploratory Data Analysis (EDA) would typically be done to confirm this decision.\n",
    "#### Target\n",
    "`survived` is our target class variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'class', 'who', 'adult_male', 'alone']\n",
    "target = 'survived'\n",
    "\n",
    "X = titanic[features]\n",
    "y = titanic[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. How balanced are the classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Split the data into training and testing sets\n",
    "Don't forget to consider imbalance in the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define preprocessing transformers for numerical and categorical features\n",
    "#### Automatically detect numerical and categorical columns and assign them to separate numeric and categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define separate preprocessing pipelines for both feature types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the transformers into a single column transformer\n",
    "We'll use the sklearn \"column transformer\" estimator to separately transform the features, which will then concatenate the output as a single feature space, ready for input to a machine learning estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model pipeline\n",
    "Now let's complete the model pipeline by combining the preprocessing with a Random Forest classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a parameter grid \n",
    "We'll use the grid in a cross validation search to optimize the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform grid search cross-validation and fit the best model to the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation method\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3. Train the pipeline model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "model = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=cv, scoring='accuracy', verbose=2)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4. Get the model predictions from the grid search estimator on the unseen data\n",
    "Also print a classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. Plot the confusion matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Titanic Classification Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances\n",
    "Let's figure out how to get the feature importances of our overall model.  \n",
    "First, to obtain the categorical feature importances, we have to work our way backward through the modelling pipeline to associate the feature importances with their one-hot encoded input features that were transformed from the original categorical features.\n",
    "\n",
    "We don't need to trace back through the pipeline for the numerical features, because we didn't transfrom them into new ones in any way.  \n",
    "Remember, we went from categorical features to one-hot encoded features, using the 'cat' column transformer.\n",
    "\n",
    "Here's how you trace back through the trained model to access the one-hot encoded feature names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_estimator_['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the one-hot encoded features are named - for example, `sex` was split into two boolean features indicating whether the sex is male or female.\n",
    "\n",
    "Great! Now let's get all of the feature importances and associate them with their transformed feature names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.best_estimator_['classifier'].feature_importances_\n",
    "\n",
    "# Combine the numerical and one-hot encoded categorical feature names\n",
    "feature_names = numerical_features + list(model.best_estimator_['preprocessor']\n",
    "                                        .named_transformers_['cat']\n",
    "                                        .named_steps['onehot']\n",
    "                                        .get_feature_names_out(categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the feature importances in a bar plot\n",
    "Define a feature importance DataFrame, then plot it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({'Feature': feature_names,\n",
    "                              'Importance': feature_importances\n",
    "                             }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis() \n",
    "plt.title('Most Important Features in predicting whether a passenger survived')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()\n",
    "\n",
    "# Print test score \n",
    "test_score = model.score(X_test, y_test)\n",
    "print(f\"\\nTest set accuracy: {test_score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6. These are interesting results to consider. \n",
    "What can you say about these feature importances? Are they informative as is?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try another model\n",
    "In practice you would want to try out different models and even revisit the data analysis to improve\n",
    "your model performance. Maybe you can engineer new features or impute missing values to be able to use more data.\n",
    "\n",
    "With Scikit-learn's powerful pipeline class, this is easy to do in a few steps.\n",
    "Let's update the pipeline and the parameter grid so we can train a Logistic Regression model and compare the performance of the two models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace RandomForestClassifier with LogisticRegression\n",
    "pipeline.set_params(classifier=LogisticRegression(random_state=42))\n",
    "\n",
    "# update the model's estimator to use the new pipeline\n",
    "model.estimator = pipeline\n",
    "\n",
    "# Define a new grid with Logistic Regression parameters\n",
    "param_grid = {\n",
    "    # 'classifier__n_estimators': [50, 100],\n",
    "    # 'classifier__max_depth': [None, 10, 20],\n",
    "    # 'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__solver' : ['liblinear'],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__class_weight' : [None, 'balanced']\n",
    "}\n",
    "\n",
    "model.param_grid = param_grid\n",
    "\n",
    "# Fit the updated pipeline with Logistic Regression\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7. Display the clasification report for the new model and compare the results to your previous model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8. Display the confusion matrix for the new model and compare the results to your previous model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "# Generate the confusion matrix \n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Titanic Classification Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What changed in the numbers of true positives and true negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the logistic regression feature coefficients and plot their magnitude in a bar chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = model.best_estimator_.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Combine numerical and categorical feature names\n",
    "numerical_feature_names = numerical_features\n",
    "categorical_feature_names = (model.best_estimator_.named_steps['preprocessor']\n",
    "                                     .named_transformers_['cat']\n",
    "                                     .named_steps['onehot']\n",
    "                                     .get_feature_names_out(categorical_features)\n",
    "                            )\n",
    "feature_names = numerical_feature_names + list(categorical_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9. Plot the feature coefficient magnitudes in a bar chart\n",
    "What's different about this chart than the feature importance chart for the Random Forest classifier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "\n",
    "# Create a DataFrame for the coefficients\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "}).sort_values(by='Coefficient', ascending=False, key=abs)  # Sort by absolute values\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Coefficient'].abs(), color='skyblue')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Feature Coefficient magnitudes for Logistic Regression model')\n",
    "plt.xlabel('Coefficient Magnitude')\n",
    "plt.show()\n",
    "\n",
    "# Print test score\n",
    "test_score = model.best_estimator_.score(X_test, y_test)\n",
    "print(f\"\\nTest set accuracy: {test_score:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "prev_pub_hash": "faaef4beb1782e94a0fb800fdc7e7929179d4ae69837318059cd613f612726b4"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
